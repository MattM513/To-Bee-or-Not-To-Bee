{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a69c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Predictions for \"To bee or not to bee\"\n",
    "# Generate predictions for images 251-347\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from skimage import measure, feature\n",
    "from scipy.optimize import minimize\n",
    "from skimage.transform import rotate\n",
    "\n",
    "# Import your feature extraction functions here\n",
    "# You need to copy the functions from your feature_extraction notebook\n",
    "\n",
    "# ==================== COPY YOUR FEATURE EXTRACTION FUNCTIONS HERE ====================\n",
    "# Copy these functions from your feature_extraction.ipynb:\n",
    "# - load_image_and_mask\n",
    "# - compute_best_inscribed_circle\n",
    "# - compute_best_symmetry_plane\n",
    "# - extract_shape_features\n",
    "# - calculate_bug_ratio\n",
    "# - extract_color_features\n",
    "# - extract_additional_features\n",
    "# - create_default_features\n",
    "\n",
    "# For now, I'll provide a template function\n",
    "def extract_all_features_test(image_id, test_dir=\"../test\"):\n",
    "    \"\"\"\n",
    "    Extract all features for a test image\n",
    "    This should be identical to your training feature extraction\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image and mask\n",
    "        image_path = os.path.join(test_dir, \"images\", f\"{image_id}.jpg\")\n",
    "        mask_path = os.path.join(test_dir, \"masks\", f\"binary_{image_id}.tif\")\n",
    "        \n",
    "        # Check if files exist\n",
    "        if not os.path.exists(image_path) or not os.path.exists(mask_path):\n",
    "            print(f\"Missing files for image {image_id}\")\n",
    "            return create_default_features(image_id)\n",
    "        \n",
    "        # Load and process (you need to use your actual functions here)\n",
    "        image, mask = load_image_and_mask(image_id, visualize=False)\n",
    "        \n",
    "        # Extract all features\n",
    "        shape_features = extract_shape_features(mask)\n",
    "        bug_ratio = calculate_bug_ratio(mask)\n",
    "        color_features = extract_color_features(image, mask)\n",
    "        additional_features = extract_additional_features(image, mask)\n",
    "        \n",
    "        # Compute inscribed circle and symmetry\n",
    "        max_radius, circle_center = compute_best_inscribed_circle(mask)\n",
    "        \n",
    "        if max_radius > 5:\n",
    "            symmetry_score, symmetry_angle = compute_best_symmetry_plane(\n",
    "                image, mask, circle_center, max_radius\n",
    "            )\n",
    "        else:\n",
    "            symmetry_score = 0.0\n",
    "            symmetry_angle = 0.0\n",
    "        \n",
    "        # Combine all features\n",
    "        features = {\n",
    "            'image_id': int(image_id),\n",
    "            'bug_ratio': bug_ratio,\n",
    "            'max_inscribed_radius': max_radius,\n",
    "            'symmetry_score': symmetry_score,\n",
    "            'symmetry_angle': symmetry_angle,\n",
    "            **shape_features,\n",
    "            **color_features,\n",
    "            **additional_features\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_id}: {e}\")\n",
    "        return create_default_features(image_id)\n",
    "\n",
    "# ==================== MAIN PREDICTION PIPELINE ====================\n",
    "\n",
    "def main():\n",
    "    print(\"=== TEST SET PREDICTIONS (Images 251-347) ===\\n\")\n",
    "    \n",
    "    # 1. Load saved models and preprocessors\n",
    "    print(\"1. Loading models and preprocessors...\")\n",
    "    try:\n",
    "        best_model = joblib.load('models/best_model.pkl')\n",
    "        scaler = joblib.load('../scaler.pkl')  # From feature extraction\n",
    "        label_encoder = joblib.load('models/label_encoder.pkl')\n",
    "        print(\"‚úì Models loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading models: {e}\")\n",
    "        print(\"Make sure you have run the ML notebook and feature extraction first!\")\n",
    "        return\n",
    "    \n",
    "    # 2. Get feature columns from training data\n",
    "    train_features = pd.read_csv('../features_normalized.csv')\n",
    "    feature_cols = [col for col in train_features.columns \n",
    "                   if col not in ['image_id', 'bug_type', 'species']]\n",
    "    print(f\"‚úì Using {len(feature_cols)} features\")\n",
    "    \n",
    "    # 3. Extract features for test images\n",
    "    print(\"\\n2. Extracting features for test images...\")\n",
    "    test_features = []\n",
    "    failed_images = []\n",
    "    \n",
    "    for image_id in tqdm(range(251, 348), desc=\"Processing images\"):\n",
    "        features = extract_all_features_test(image_id)\n",
    "        if features is not None:\n",
    "            test_features.append(features)\n",
    "        else:\n",
    "            failed_images.append(image_id)\n",
    "    \n",
    "    print(f\"‚úì Successfully processed {len(test_features)} images\")\n",
    "    if failed_images:\n",
    "        print(f\"‚ö†Ô∏è  Failed to process {len(failed_images)} images: {failed_images}\")\n",
    "    \n",
    "    # 4. Create DataFrame and ensure column consistency\n",
    "    test_df = pd.DataFrame(test_features)\n",
    "    \n",
    "    # Ensure all required columns are present\n",
    "    missing_cols = set(feature_cols) - set(test_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"\\n‚ö†Ô∏è  Adding missing columns with default values: {missing_cols}\")\n",
    "        for col in missing_cols:\n",
    "            test_df[col] = 0.0\n",
    "    \n",
    "    # Select features in the same order as training\n",
    "    X_test = test_df[feature_cols].values\n",
    "    \n",
    "    # 5. Normalize features\n",
    "    print(\"\\n3. Normalizing features...\")\n",
    "    X_test_normalized = scaler.transform(X_test)\n",
    "    \n",
    "    # 6. Generate predictions\n",
    "    print(\"\\n4. Generating predictions...\")\n",
    "    predictions_encoded = best_model.predict(X_test_normalized)\n",
    "    predictions = label_encoder.inverse_transform(predictions_encoded)\n",
    "    \n",
    "    # 7. Create submission DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'ID': test_df['image_id'].astype(int),\n",
    "        'bug type': predictions\n",
    "    })\n",
    "    \n",
    "    # Sort by ID to ensure correct order\n",
    "    submission_df = submission_df.sort_values('ID')\n",
    "    \n",
    "    # 8. Display prediction summary\n",
    "    print(\"\\n5. Prediction Summary:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(submission_df['bug type'].value_counts())\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check for any suspicious predictions\n",
    "    if 'Bee & Bumblebee' in submission_df['bug type'].values:\n",
    "        count = (submission_df['bug type'] == 'Bee & Bumblebee').sum()\n",
    "        print(f\"\\n‚ö†Ô∏è  Warning: {count} images predicted as 'Bee & Bumblebee'\")\n",
    "        print(\"This rare class had only 1 training sample - predictions may be unreliable\")\n",
    "    \n",
    "    # 9. Save submission file\n",
    "    output_path = '../predictions_test.csv'\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n‚úì Predictions saved to: {output_path}\")\n",
    "    print(f\"‚úì File contains {len(submission_df)} predictions\")\n",
    "    \n",
    "    # 10. Verify submission format\n",
    "    print(\"\\n6. Submission file verification:\")\n",
    "    print(f\"  - Columns: {list(submission_df.columns)}\")\n",
    "    print(f\"  - ID range: {submission_df['ID'].min()} to {submission_df['ID'].max()}\")\n",
    "    print(f\"  - Unique bug types: {sorted(submission_df['bug type'].unique())}\")\n",
    "    print(f\"  - Shape: {submission_df.shape}\")\n",
    "    \n",
    "    # Display first few predictions\n",
    "    print(\"\\nFirst 10 predictions:\")\n",
    "    print(submission_df.head(10))\n",
    "    \n",
    "    print(\"\\n‚úÖ TEST PREDICTIONS COMPLETE!\")\n",
    "    print(f\"üìÅ Submit the file: {output_path}\")\n",
    "\n",
    "# ==================== ALTERNATIVE: BATCH PREDICTION ====================\n",
    "\n",
    "def predict_from_existing_features(test_features_path):\n",
    "    \"\"\"\n",
    "    Alternative method if you've already extracted test features\n",
    "    \"\"\"\n",
    "    print(\"=== PREDICTIONS FROM EXISTING FEATURES ===\\n\")\n",
    "    \n",
    "    # Load models\n",
    "    best_model = joblib.load('models/best_model.pkl')\n",
    "    scaler = joblib.load('../scaler.pkl')\n",
    "    label_encoder = joblib.load('models/label_encoder.pkl')\n",
    "    \n",
    "    # Load test features\n",
    "    test_df = pd.read_csv(test_features_path)\n",
    "    \n",
    "    # Get feature columns\n",
    "    train_features = pd.read_csv('../features_normalized.csv')\n",
    "    feature_cols = [col for col in train_features.columns \n",
    "                   if col not in ['image_id', 'bug_type', 'species']]\n",
    "    \n",
    "    # Prepare data\n",
    "    X_test = test_df[feature_cols].values\n",
    "    X_test_normalized = scaler.transform(X_test)\n",
    "    \n",
    "    # Predict\n",
    "    predictions_encoded = best_model.predict(X_test_normalized)\n",
    "    predictions = label_encoder.inverse_transform(predictions_encoded)\n",
    "    \n",
    "    # Create submission\n",
    "    submission_df = pd.DataFrame({\n",
    "        'ID': test_df['image_id'].astype(int),\n",
    "        'bug type': predictions\n",
    "    })\n",
    "    \n",
    "    submission_df.to_csv('../predictions_test.csv', index=False)\n",
    "    print(f\"‚úì Predictions saved to: ../predictions_test.csv\")\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "# ==================== RUN THE PREDICTION ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Option 1: Extract features and predict\n",
    "    main()\n",
    "    \n",
    "    # Option 2: If you have already extracted test features to a CSV file\n",
    "    # test_features_path = '../test_features.csv'\n",
    "    # if os.path.exists(test_features_path):\n",
    "    #     predict_from_existing_features(test_features_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
